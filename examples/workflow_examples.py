"""
å·¥ä½œæµç¤ºä¾‹æ¨¡å—

æœ¬æ¨¡å—å±•ç¤ºå®Œæ•´çš„AIè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼Œå°†å„ä¸ªåŠŸèƒ½æ¨¡å—ç»„åˆèµ·æ¥å®Œæˆç«¯åˆ°ç«¯ä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡æœ¬åˆ†ç±»å·¥ä½œæµ
2. æ•°æ®åˆ†æå·¥ä½œæµ
3. å†…å®¹ç”Ÿæˆå·¥ä½œæµ
4. å¼‚å¸¸å¤„ç†å·¥ä½œæµ
5. è‡ªå®šä¹‰å·¥ä½œæµæ„å»º

è¿è¡Œç¯å¢ƒè¦æ±‚ï¼š
- Python 3.8+
- å·²å®‰è£…æ‰€æœ‰ä¾èµ–åº“
- é…ç½®æ–‡ä»¶æ­£ç¡®è®¾ç½®
- æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½åˆ°é¡¹ç›®æ¨¡å‹ç›®å½•

é¢„æœŸè¾“å‡ºï¼š
å±•ç¤ºå®Œæ•´å·¥ä½œæµçš„è¿è¡Œè¿‡ç¨‹å’Œç»“æœ
"""

import os
import sys
import time
import json
import logging
import pandas as pd
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥é¡¹ç›®ä¸­çš„æ¨¡å—ï¼ˆæ ¹æ®å®é™…é¡¹ç›®ç»“æ„è°ƒæ•´å¯¼å…¥è·¯å¾„ï¼‰
from myproject.core import model, data_processor, post_processor
from myproject.api import client
from myproject.utils import config, file_utils
from myproject.workflow import workflow_manager, task
from myproject.exceptions import WorkflowError

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_all():
    """è¿è¡Œæ‰€æœ‰å·¥ä½œæµç¤ºä¾‹"""
    print("å¼€å§‹è¿è¡Œå·¥ä½œæµç¤ºä¾‹...")
    
    # è¿è¡Œæ–‡æœ¬åˆ†ç±»å·¥ä½œæµç¤ºä¾‹
    run_text_classification_workflow()
    
    # è¿è¡Œæ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹
    run_data_analysis_workflow()
    
    # è¿è¡Œå†…å®¹ç”Ÿæˆå·¥ä½œæµç¤ºä¾‹
    run_content_generation_workflow()
    
    # è¿è¡Œé”™è¯¯å¤„ç†å·¥ä½œæµç¤ºä¾‹
    run_error_handling_workflow()
    
    # è¿è¡Œè‡ªå®šä¹‰å·¥ä½œæµæ„å»ºç¤ºä¾‹
    run_custom_workflow_builder_example()
    
    print("å·¥ä½œæµç¤ºä¾‹è¿è¡Œå®Œæˆï¼")

def run_text_classification_workflow():
    """
    æ–‡æœ¬åˆ†ç±»å·¥ä½œæµç¤ºä¾‹
    
    å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬åˆ†ç±»æµç¨‹
    """
    print("\n--- æ–‡æœ¬åˆ†ç±»å·¥ä½œæµç¤ºä¾‹ ---")
    
    try:
        # 1. å®šä¹‰å·¥ä½œæµé…ç½®
        workflow_config = {
            "name": "text_classification_workflow",
            "description": "æ–‡æœ¬åˆ†ç±»å®Œæ•´å·¥ä½œæµ",
            "model": {
                "name": "text-classifier",
                "path": "./models/text-classifier",
                "max_length": 256
            },
            "data": {
                "input_format": "csv",
                "text_column": "text",
                "batch_size": 16
            },
            "output": {
                "format": "json",
                "path": "./output/classification_results.json"
            }
        }
        
        print("åˆå§‹åŒ–æ–‡æœ¬åˆ†ç±»å·¥ä½œæµ")
        
        # 2. åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = workflow_manager.WorkflowManager(workflow_config)
        
        # 3. å®šä¹‰å’Œæ·»åŠ å·¥ä½œæµä»»åŠ¡
        
        # 3.1 å‡†å¤‡ä¸»é¢˜ä»»åŠ¡
        @workflow.task(name="prepare_topic")
        def prepare_generation_topic():
            print("æ‰§è¡Œä¸»é¢˜å‡†å¤‡ä»»åŠ¡...")
            
            # æ¨¡æ‹Ÿè¾“å…¥ä¸»é¢˜å’Œå…³é”®è¯
            topic_data = {
                "main_topic": "äººå·¥æ™ºèƒ½åº”ç”¨",
                "subtopics": ["è‡ªç„¶è¯­è¨€å¤„ç†", "è®¡ç®—æœºè§†è§‰", "æ¨èç³»ç»Ÿ"],
                "keywords": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æœºå™¨å­¦ä¹ ", "å¤§æ¨¡å‹", "è‡ªåŠ¨åŒ–"],
                "target_audience": "æŠ€æœ¯çˆ±å¥½è€…å’Œä¼ä¸šå†³ç­–è€…",
                "tone": "ä¸“ä¸šä½†æ˜“äºç†è§£"
            }
            
            print(f"ä¸»é¢˜å‡†å¤‡å®Œæˆ: {topic_data['main_topic']}")
            return topic_data
        
        # 3.2 å†…å®¹ç”Ÿæˆä»»åŠ¡
        @workflow.task(name="generate_content", depends_on=["prepare_topic"])
        def generate_content(topic_data):
            print("æ‰§è¡Œå†…å®¹ç”Ÿæˆä»»åŠ¡...")
            
            # æ¨¡æ‹Ÿå†…å®¹ç”Ÿæˆæ¨¡å‹
            print(f"ä½¿ç”¨æ¨¡å‹ {workflow_config['model']['name']} ç”Ÿæˆå†…å®¹")
            print(f"ä¸»é¢˜: {topic_data['main_topic']}")
            print(f"å­ä¸»é¢˜: {', '.join(topic_data['subtopics'])}")
            print(f"ç”Ÿæˆå‚æ•°: æ¸©åº¦={workflow_config['generation']['temperature']}, top_p={workflow_config['generation']['top_p']}")
            
            # ä¸ºä¸åŒå†…å®¹ç±»å‹ç”Ÿæˆå†…å®¹
            generated_contents = {}
            
            # 1. ç”Ÿæˆæ–‡ç« å†…å®¹
            article_content = f"""
            # {topic_data['main_topic']}ï¼šæŠ€æœ¯ä¸åº”ç”¨

            åœ¨å½“ä»Šæ•°å­—åŒ–æ—¶ä»£ï¼Œ{topic_data['main_topic']}æ­£åœ¨å„ä¸ªé¢†åŸŸæ€èµ·å˜é©æµªæ½®ã€‚æœ¬æ–‡å°†æ¢è®¨ä¸‰ä¸ªå…³é”®é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼š{topic_data['subtopics'][0]}ã€{topic_data['subtopics'][1]}å’Œ{topic_data['subtopics'][2]}ã€‚

            ## {topic_data['subtopics'][0]}
            {topic_data['subtopics'][0]}æ˜¯{topic_data['main_topic']}ä¸­æœ€æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸä¹‹ä¸€ã€‚å€ŸåŠ©{topic_data['keywords'][0]}å’Œ{topic_data['keywords'][1]}æŠ€æœ¯ï¼Œç°ä»£{topic_data['subtopics'][0]}ç³»ç»Ÿèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œä¸ºæ™ºèƒ½å®¢æœã€è‡ªåŠ¨ç¿»è¯‘å’Œå†…å®¹åˆ›ä½œç­‰åº”ç”¨æä¾›å¼ºå¤§æ”¯æŒã€‚

            ## {topic_data['subtopics'][1]}
            {topic_data['subtopics'][1]}æŠ€æœ¯è®©è®¡ç®—æœºæ‹¥æœ‰äº†"çœ‹"çš„èƒ½åŠ›ã€‚é€šè¿‡{topic_data['keywords'][1]}æŠ€æœ¯ï¼Œè®¡ç®—æœºå¯ä»¥è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ã€äººè„¸å’Œåœºæ™¯ï¼Œå¹¿æ³›åº”ç”¨äºå®‰é˜²ç›‘æ§ã€åŒ»ç–—è¯Šæ–­å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚

            ## {topic_data['subtopics'][2]}
            éšç€ç”µå­å•†åŠ¡å’Œå†…å®¹å¹³å°çš„å…´èµ·ï¼Œ{topic_data['subtopics'][2]}å˜å¾—æ„ˆå‘é‡è¦ã€‚åŸºäº{topic_data['keywords'][2]}çš„{topic_data['subtopics'][2]}èƒ½å¤Ÿåˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„äº§å“å’Œå†…å®¹æ¨èï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œå¹³å°æ•ˆç›Šã€‚

            # æœªæ¥å±•æœ›
            éšç€{topic_data['keywords'][3]}çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬é¢„è®¡æœªæ¥{topic_data['main_topic']}å°†å‘æ›´é«˜ç²¾åº¦ã€æ›´å¼ºè§£é‡Šæ€§å’Œæ›´å¹¿åº”ç”¨åœºæ™¯æ–¹å‘å‘å±•ï¼Œä¸ºä¼ä¸šå’Œç¤¾ä¼šåˆ›é€ æ›´å¤§ä»·å€¼ã€‚
            """
            generated_contents["article"] = article_content
            
            # 2. ç”Ÿæˆç¤¾äº¤åª’ä½“å†…å®¹
            social_media_content = f"""
            ğŸ“± #{topic_data['keywords'][0]} #{topic_data['keywords'][3]} #{topic_data['main_topic'].replace(' ', '')}

            ğŸ” æƒ³äº†è§£{topic_data['main_topic']}å¦‚ä½•æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œå—ï¼Ÿ
            
            ğŸš€ ä»{topic_data['subtopics'][0]}åˆ°{topic_data['subtopics'][1]}ï¼Œå†åˆ°{topic_data['subtopics'][2]}ï¼ŒAIæ­£åœ¨å„ä¸ªé¢†åŸŸæ€èµ·é©å‘½ï¼
            
            ğŸ’¡ æ— è®ºæ˜¯æå‡æ•ˆç‡è¿˜æ˜¯åˆ›é€ æ–°å¯èƒ½ï¼Œ{topic_data['main_topic']}éƒ½å°†æ˜¯æœªæ¥æŠ€æœ¯å‘å±•çš„æ ¸å¿ƒé©±åŠ¨åŠ›ã€‚
            
            ğŸ‘‰ ç‚¹å‡»é“¾æ¥äº†è§£æ›´å¤šå…³äº{topic_data['main_topic']}çš„æœ€æ–°ç ”ç©¶å’Œåº”ç”¨æ¡ˆä¾‹ï¼#æŠ€æœ¯åˆ›æ–°
            """
            generated_contents["social_media"] = social_media_content
            
            # 3. ç”Ÿæˆé‚®ä»¶è¥é”€å†…å®¹
            email_content = f"""
            ä¸»é¢˜: æ¢ç´¢{topic_data['main_topic']}ä¸ºæ‚¨çš„ä¸šåŠ¡å¸¦æ¥çš„é©å‘½æ€§å˜åŒ–
            
            å°Šæ•¬çš„å†³ç­–è€…ï¼š
            
            å¸Œæœ›è¿™å°é‚®ä»¶æ‰¾åˆ°æ‚¨ä¸€åˆ‡å®‰å¥½ã€‚
            
            åœ¨å½“ä»Šç«äº‰æ¿€çƒˆçš„å•†ä¸šç¯å¢ƒä¸­ï¼Œ{topic_data['main_topic']}æ­£åœ¨æˆä¸ºä¼ä¸šä¿æŒç«äº‰åŠ›çš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬å›¢é˜Ÿä¸“æ³¨äº{topic_data['subtopics'][0]}ã€{topic_data['subtopics'][1]}å’Œ{topic_data['subtopics'][2]}ç­‰é¢†åŸŸçš„å‰æ²¿æŠ€æœ¯å¼€å‘ï¼Œå¸®åŠ©ä¼ä¸šå®ç°æ•°å­—åŒ–è½¬å‹ã€‚
            
            æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŸºäºæœ€æ–°çš„{topic_data['keywords'][0]}å’Œ{topic_data['keywords'][3]}æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¸®åŠ©æ‚¨ï¼š
            
            - ä¼˜åŒ–ä¸šåŠ¡æµç¨‹ï¼Œæé«˜è¿è¥æ•ˆç‡
            - å¢å¼ºå®¢æˆ·ä½“éªŒï¼Œæé«˜å®¢æˆ·æ»¡æ„åº¦
            - æŒ–æ˜æ•°æ®ä»·å€¼ï¼Œæ”¯æŒç²¾å‡†å†³ç­–
            
            å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šå…³äº{topic_data['main_topic']}å¦‚ä½•ä¸ºæ‚¨çš„ä¸šåŠ¡åˆ›é€ ä»·å€¼ï¼Œè¯·å›å¤æ­¤é‚®ä»¶æˆ–è‡´ç”µæˆ‘ä»¬ï¼Œæˆ‘ä»¬å¾ˆä¹æ„å®‰æ’ä¸€æ¬¡æ·±å…¥äº¤æµã€‚
            
            æœŸå¾…æ‚¨çš„å›éŸ³ï¼
            
            æ­¤è‡´
            
            AIè§£å†³æ–¹æ¡ˆå›¢é˜Ÿ
            """
            generated_contents["email"] = email_content
            
            print(f"æˆåŠŸç”Ÿæˆ{len(generated_contents)}ç§ç±»å‹çš„å†…å®¹")
            return generated_contents
        
        # 3.3 å†…å®¹ä¼˜åŒ–ä»»åŠ¡
        @workflow.task(name="optimize_content", depends_on=["generate_content"])
        def optimize_content(contents):
            print("æ‰§è¡Œå†…å®¹ä¼˜åŒ–ä»»åŠ¡...")
            
            optimization_results = {}
            
            for content_type, content in contents.items():
                print(f"ä¼˜åŒ–{content_type}ç±»å‹å†…å®¹...")
                
                # æ¨¡æ‹Ÿå†…å®¹ä¼˜åŒ–è¿‡ç¨‹
                optimized_content = content
                optimization_metrics = {}
                
                # 1. å¯è¯»æ€§æ£€æŸ¥
                if workflow_config["optimization"]["readability_check"]:
                    # æ¨¡æ‹Ÿå¯è¯»æ€§åˆ†æ
                    readability_score = 75 + (hash(content) % 15)  # ç”Ÿæˆ65-90ä¹‹é—´çš„æ¨¡æ‹Ÿåˆ†æ•°
                    optimization_metrics["readability"] = readability_score
                    print(f"  å¯è¯»æ€§åˆ†æ•°: {readability_score}")
                    
                    # åŸºäºåˆ†æ•°è¿›è¡Œä¼˜åŒ–ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼‰
                    if readability_score < 70:
                        print("  åº”ç”¨å¯è¯»æ€§ä¼˜åŒ–ï¼šç®€åŒ–å¥å­ç»“æ„ï¼Œå‡å°‘ä¸“ä¸šæœ¯è¯­")
                        # å®é™…é¡¹ç›®ä¸­ä¼šæœ‰çœŸå®çš„ä¼˜åŒ–é€»è¾‘
                
                # 2. æƒ…æ„Ÿåˆ†æ
                if workflow_config["optimization"]["sentiment_analysis"]:
                    # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æ
                    sentiment_score = 0.6 + (hash(content) % 5) / 10  # ç”Ÿæˆ0.6-1.0ä¹‹é—´çš„æ¨¡æ‹Ÿåˆ†æ•°
                    sentiment_label = "ç§¯æ" if sentiment_score > 0.7 else "ä¸­æ€§"
                    optimization_metrics["sentiment"] = {
                        "score": sentiment_score,
                        "label": sentiment_label
                    }
                    print(f"  æƒ…æ„Ÿåˆ†æ: {sentiment_label} (åˆ†æ•°: {sentiment_score:.2f})")
                
                # 3. è¯­æ³•æ£€æŸ¥
                if workflow_config["optimization"]["grammar_check"]:
                    # æ¨¡æ‹Ÿè¯­æ³•æ£€æŸ¥
                    grammar_errors = abs(hash(content) % 5)  # ç”Ÿæˆ0-4ä¹‹é—´çš„æ¨¡æ‹Ÿé”™è¯¯æ•°
                    optimization_metrics["grammar_errors"] = grammar_errors
                    print(f"  è¯­æ³•é”™è¯¯æ•°: {grammar_errors}")
                    
                    if grammar_errors > 0:
                        print(f"  ä¿®å¤äº†{grammar_errors}ä¸ªè¯­æ³•é”™è¯¯")
                        # å®é™…é¡¹ç›®ä¸­ä¼šæœ‰çœŸå®çš„è¯­æ³•ä¿®å¤é€»è¾‘
                
                # å­˜å‚¨ä¼˜åŒ–ç»“æœ
                optimization_results[content_type] = {
                    "original": content,
                    "optimized": optimized_content,
                    "metrics": optimization_metrics
                }
            
            print("å†…å®¹ä¼˜åŒ–å®Œæˆ")
            return optimization_results
        
        # 3.4 å†…å®¹å¯¼å‡ºä»»åŠ¡
        @workflow.task(name="export_content", depends_on=["optimize_content"])
        def export_content(optimization_results):
            print("æ‰§è¡Œå†…å®¹å¯¼å‡ºä»»åŠ¡...")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = "./output/generated_content/"
            os.makedirs(output_dir, exist_ok=True)
            
            exported_files = []
            
            # å¯¼å‡ºå„ç±»å‹å†…å®¹
            for content_type, result in optimization_results.items():
                # ç¡®å®šæ–‡ä»¶åå’Œæ ¼å¼
                if content_type == "article":
                    file_path = os.path.join(output_dir, "article.md")
                    content = result["optimized"]
                elif content_type == "social_media":
                    file_path = os.path.join(output_dir, "social_media_post.txt")
                    content = result["optimized"]
                elif content_type == "email":
                    file_path = os.path.join(output_dir, "marketing_email.txt")
                    content = result["optimized"]
                else:
                    file_path = os.path.join(output_dir, f"{content_type}_content.txt")
                    content = result["optimized"]
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"å·²å¯¼å‡º{content_type}å†…å®¹åˆ°: {file_path}")
                exported_files.append(file_path)
            
            # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            summary_path = os.path.join(output_dir, "generation_summary.json")
            
            summary = {
                "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "content_types": list(optimization_results.keys()),
                "optimization_metrics": {
                    content_type: result["metrics"]
                    for content_type, result in optimization_results.items()
                },
                "exported_files": exported_files
            }
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2)
            
            print(f"å·²ç”Ÿæˆå†…å®¹æ‘˜è¦æŠ¥å‘Š: {summary_path}")
            return {
                "exported_files": exported_files,
                "summary_path": summary_path
            }
        
        # 4. æ‰§è¡Œå·¥ä½œæµ
        print("\nå¼€å§‹æ‰§è¡Œå†…å®¹ç”Ÿæˆå·¥ä½œæµ...")
        result = workflow.run()
        
        # 5. æ€»ç»“å·¥ä½œæµæ‰§è¡Œç»“æœ
        print("\nå†…å®¹ç”Ÿæˆå·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {workflow.execution_time:.2f}ç§’")
        print(f"ä»»åŠ¡æ‰§è¡Œé¡ºåº: {', '.join(workflow.execution_order)}")
        print(f"ç”Ÿæˆçš„å†…å®¹ç±»å‹: {', '.join(list(optimization_results.keys()))}")
        print(f"å¯¼å‡ºçš„æ–‡ä»¶æ•°é‡: {len(result['exported_files'])}")
        print(f"æ‘˜è¦æŠ¥å‘Šè·¯å¾„: {result['summary_path']}")
        
    except WorkflowError as e:
        print(f"å·¥ä½œæµé”™è¯¯: {e}")
    except Exception as e:
        print(f"æœªé¢„æœŸé”™è¯¯: {e}")

def run_error_handling_workflow():
    """
    é”™è¯¯å¤„ç†å·¥ä½œæµç¤ºä¾‹
    
    å±•ç¤ºå¦‚ä½•åœ¨å·¥ä½œæµä¸­å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
    """
    print("\n--- é”™è¯¯å¤„ç†å·¥ä½œæµç¤ºä¾‹ ---")
    
    try:
        # 1. å®šä¹‰å·¥ä½œæµé…ç½®
        workflow_config = {
            "name": "error_handling_workflow",
            "description": "é”™è¯¯å¤„ç†ç¤ºä¾‹å·¥ä½œæµ",
            "error_handling": {
                "max_retries": 3,
                "retry_delay": 2,  # ç§’
                "fallback_enabled": True
            }
        }
        
        print("åˆå§‹åŒ–é”™è¯¯å¤„ç†å·¥ä½œæµ")
        
        # 2. åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = workflow_manager.WorkflowManager(workflow_config)
        
        # 3. å®šä¹‰å’Œæ·»åŠ å·¥ä½œæµä»»åŠ¡
        
        # 3.1 æ­£å¸¸ä»»åŠ¡
        @workflow.task(name="normal_task")
        def run_normal_task():
            print("æ‰§è¡Œæ­£å¸¸ä»»åŠ¡...")
            
            # æ¨¡æ‹Ÿæ­£å¸¸ä»»åŠ¡å¤„ç†
            data = {"status": "success", "value": 42}
            
            print(f"æ­£å¸¸ä»»åŠ¡å®Œæˆ: {data}")
            return data
        
        # 3.2 æœ‰æ—¶ä¼šå¤±è´¥çš„ä»»åŠ¡
        @workflow.task(name="intermittent_failure_task", depends_on=["normal_task"])
        def run_intermittent_failure(input_data):
            print("æ‰§è¡Œé—´æ­‡æ€§å¤±è´¥ä»»åŠ¡...")
            
            # è·å–æœ€å¤§é‡è¯•æ¬¡æ•°
            max_retries = workflow_config["error_handling"]["max_retries"]
            
            # æ¨¡æ‹Ÿä»»åŠ¡æœ‰æ—¶ä¼šå¤±è´¥
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œå¯èƒ½æ˜¯ç½‘ç»œè¯·æ±‚ã€æ•°æ®åº“æ“ä½œç­‰å¯èƒ½ä¼šå¤±è´¥çš„æ“ä½œ
            
            # å½“å‰é‡è¯•æ¬¡æ•°ï¼ˆä»»åŠ¡çŠ¶æ€å˜é‡ï¼‰
            if not hasattr(run_intermittent_failure, "retry_count"):
                run_intermittent_failure.retry_count = 0
            
            # æ¨¡æ‹Ÿå¤±è´¥æƒ…å†µ
            if run_intermittent_failure.retry_count < 2:  # å‰ä¸¤æ¬¡å¤±è´¥ï¼Œç¬¬ä¸‰æ¬¡æˆåŠŸ
                run_intermittent_failure.retry_count += 1
                failure_chance = 1.0  # 100%å¤±è´¥
            else:
                failure_chance = 0.0  # 0%å¤±è´¥ï¼ˆå¿…å®šæˆåŠŸï¼‰
            
            # æ ¹æ®å¤±è´¥æ¦‚ç‡å†³å®šæ˜¯å¦æŠ›å‡ºå¼‚å¸¸
            if random.random() < failure_chance:
                error_message = "æ¨¡æ‹Ÿçš„é—´æ­‡æ€§ä»»åŠ¡å¤±è´¥"
                print(f"ä»»åŠ¡å¤±è´¥: {error_message} (é‡è¯• {run_intermittent_failure.retry_count}/{max_retries})")
                
                # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´å¼‚å¸¸ç±»å‹
                if run_intermittent_failure.retry_count == 1:
                    raise ConnectionError(error_message)  # æ¨¡æ‹Ÿè¿æ¥é”™è¯¯
                else:
                    raise TimeoutError(error_message)  # æ¨¡æ‹Ÿè¶…æ—¶é”™è¯¯
            
            # ä»»åŠ¡æˆåŠŸ
            result = {
                "processed_value": input_data["value"] * 2,
                "retry_count": run_intermittent_failure.retry_count
            }
            
            print(f"é—´æ­‡æ€§ä»»åŠ¡å®Œæˆ: {result}, é‡è¯•æ¬¡æ•°: {run_intermittent_failure.retry_count}")
            return result
        
        # 3.3 å§‹ç»ˆä¼šå¤±è´¥çš„ä»»åŠ¡åŠå…¶å¤‡ç”¨ä»»åŠ¡
        @workflow.task(name="always_failing_task", depends_on=["intermittent_failure_task"])
        def run_always_failing_task(input_data):
            print("æ‰§è¡Œå§‹ç»ˆå¤±è´¥ä»»åŠ¡...")
            
            # æ¨¡æ‹Ÿå§‹ç»ˆä¼šå¤±è´¥çš„ä»»åŠ¡
            error_message = "å§‹ç»ˆå¤±è´¥çš„ä»»åŠ¡é”™è¯¯"
            print(f"ä»»åŠ¡å¤±è´¥: {error_message}")
            
            # æŠ›å‡ºå¼‚å¸¸
            raise RuntimeError(error_message)
        
        # å¤‡ç”¨ä»»åŠ¡: å½“ä¸»ä»»åŠ¡å¤±è´¥æ—¶æ‰§è¡Œ
        @workflow.task(name="fallback_task", fallback_for="always_failing_task")
        def run_fallback_task(input_data):
            print("æ‰§è¡Œå¤‡ç”¨ä»»åŠ¡...")
            
            # å¤‡ç”¨å¤„ç†é€»è¾‘
            fallback_result = {
                "fallback_value": input_data["processed_value"] / 2,
                "is_fallback": True
            }
            
            print(f"å¤‡ç”¨ä»»åŠ¡å®Œæˆ: {fallback_result}")
            return fallback_result
        
        # 3.4 æ¡ä»¶åˆ†æ”¯ä»»åŠ¡
        @workflow.task(name="conditional_task", depends_on=["always_failing_task", "fallback_task"])
        def run_conditional_task(failing_task_result=None, fallback_task_result=None):
            print("æ‰§è¡Œæ¡ä»¶åˆ†æ”¯ä»»åŠ¡...")
            
            # æ£€æŸ¥å“ªä¸ªå‰ç½®ä»»åŠ¡æˆåŠŸå®Œæˆäº†
            if failing_task_result is not None:
                # å§‹ç»ˆå¤±è´¥çš„ä»»åŠ¡ç«Ÿç„¶æˆåŠŸäº†
                result_source = "always_failing_task"
                base_value = failing_task_result
            elif fallback_task_result is not None:
                # ä½¿ç”¨å¤‡ç”¨ä»»åŠ¡çš„ç»“æœ
                result_source = "fallback_task"
                base_value = fallback_task_result
            else:
                # ä¸¤ä¸ªä»»åŠ¡éƒ½å¤±è´¥äº†
                error_message = "æ‰€æœ‰å‰ç½®ä»»åŠ¡éƒ½å¤±è´¥äº†"
                print(f"æ¡ä»¶åˆ†æ”¯ä»»åŠ¡å¤±è´¥: {error_message}")
                raise ValueError(error_message)
            
            # æ ¹æ®ç»“æœæ¥æºæ‰§è¡Œä¸åŒçš„é€»è¾‘
            if result_source == "always_failing_task":
                result = {
                    "source": result_source,
                    "final_value": base_value["processed_value"] * 10,
                    "message": "ä½¿ç”¨äº†å§‹ç»ˆå¤±è´¥ä»»åŠ¡çš„ç»“æœï¼ˆæ„å¤–æƒ…å†µï¼‰"
                }
            else:
                result = {
                    "source": result_source,
                    "final_value": base_value["fallback_value"] * 5,
                    "message": "ä½¿ç”¨äº†å¤‡ç”¨ä»»åŠ¡çš„ç»“æœï¼ˆé¢„æœŸæƒ…å†µï¼‰"
                }
            
            print(f"æ¡ä»¶åˆ†æ”¯ä»»åŠ¡å®Œæˆ: {result}")
            return result
        
        # 3.5 æ¸…ç†ä»»åŠ¡(æ€»æ˜¯æ‰§è¡Œ)
        @workflow.task(name="cleanup_task", always_run=True)
        def run_cleanup_task():
            print("æ‰§è¡Œæ¸…ç†ä»»åŠ¡...")
            
            # æ¨¡æ‹Ÿèµ„æºæ¸…ç†
            print("é‡Šæ”¾èµ„æºï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            
            # è¿”å›æ¸…ç†ç»“æœ
            cleanup_result = {
                "status": "success",
                "message": "èµ„æºå·²æ¸…ç†ï¼Œä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤"
            }
            
            print(f"æ¸…ç†ä»»åŠ¡å®Œæˆ: {cleanup_result}")
            return cleanup_result
        
        # 4. æ‰§è¡Œå·¥ä½œæµ
        print("\nå¼€å§‹æ‰§è¡Œé”™è¯¯å¤„ç†å·¥ä½œæµ...")
        # æ·»åŠ é‡è¯•å’Œé”™è¯¯å¤„ç†é…ç½®
        workflow.configure_error_handling(
            retry_exceptions=[ConnectionError, TimeoutError],
            max_retries=workflow_config["error_handling"]["max_retries"],
            retry_delay=workflow_config["error_handling"]["retry_delay"],
            fallback_enabled=workflow_config["error_handling"]["fallback_enabled"]
        )
        
        result = workflow.run()
        
        # 5. æ€»ç»“å·¥ä½œæµæ‰§è¡Œç»“æœ
        print("\né”™è¯¯å¤„ç†å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {workflow.execution_time:.2f}ç§’")
        print(f"ä»»åŠ¡æ‰§è¡Œé¡ºåº: {', '.join(workflow.execution_order)}")
        
        # ç»“æœåˆ†æ
        if "conditional_task" in result:
            conditional_result = result["conditional_task"]
            print(f"æœ€ç»ˆç»“æœæ¥æº: {conditional_result['source']}")
            print(f"æœ€ç»ˆå€¼: {conditional_result['final_value']}")
            print(f"æ¶ˆæ¯: {conditional_result['message']}")
        
        if "cleanup_task" in result:
            print(f"æ¸…ç†çŠ¶æ€: {result['cleanup_task']['status']}")
        
    except WorkflowError as e:
        print(f"å·¥ä½œæµé”™è¯¯: {e}")
    except Exception as e:
        print(f"æœªé¢„æœŸé”™è¯¯: {e}")

def run_custom_workflow_builder_example():
    """
    è‡ªå®šä¹‰å·¥ä½œæµæ„å»ºç¤ºä¾‹
    
    å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä½çº§APIæ„å»ºå’Œæ‰§è¡Œè‡ªå®šä¹‰å·¥ä½œæµ
    """
    print("\n--- è‡ªå®šä¹‰å·¥ä½œæµæ„å»ºç¤ºä¾‹ ---")
    
    try:
        from myproject.workflow.task import Task
        from myproject.workflow.workflow_builder import WorkflowBuilder
        
        print("åˆå§‹åŒ–è‡ªå®šä¹‰å·¥ä½œæµæ„å»ºå™¨")
        
        # 1. åˆ›å»ºå·¥ä½œæµæ„å»ºå™¨
        builder = WorkflowBuilder(name="custom_workflow", description="è‡ªå®šä¹‰æ„å»ºçš„å·¥ä½œæµ")
        
        # 2. å®šä¹‰å·¥ä½œæµä»»åŠ¡å‡½æ•°
        
        def data_loading_func(filepath="./data/sample.csv"):
            """æ•°æ®åŠ è½½ä»»åŠ¡å‡½æ•°"""
            print(f"åŠ è½½æ•°æ®æ–‡ä»¶: {filepath}")
            # æ¨¡æ‹Ÿæ•°æ®åŠ è½½
            return {"rows": 100, "columns": 5, "status": "loaded"}
        
        def data_processing_func(data):
            """æ•°æ®å¤„ç†ä»»åŠ¡å‡½æ•°"""
            print(f"å¤„ç†æ•°æ®: {data['rows']}è¡Œ, {data['columns']}åˆ—")
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            return {"processed_items": data["rows"], "status": "processed"}
        
        def model_training_func(processed_data):
            """æ¨¡å‹è®­ç»ƒä»»åŠ¡å‡½æ•°"""
            print(f"è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨{processed_data['processed_items']}æ¡æ•°æ®")
            # æ¨¡æ‹Ÿæ¨¡å‹è®­ç»ƒ
            return {"model_accuracy": 0.92, "training_time": 120, "status": "trained"}
        
        def model_evaluation_func(model_data):
            """æ¨¡å‹è¯„ä¼°ä»»åŠ¡å‡½æ•°"""
            print(f"è¯„ä¼°æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {model_data['model_accuracy']}")
            # æ¨¡æ‹Ÿæ¨¡å‹è¯„ä¼°
            metrics = {
                "precision": 0.89,
                "recall": 0.94,
                "f1": 0.91,
                "base_accuracy": model_data["model_accuracy"]
            }
            return metrics
        
        def result_reporting_func(evaluation_metrics):
            """ç»“æœæŠ¥å‘Šä»»åŠ¡å‡½æ•°"""
            print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
            # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ
            report = {
                "metrics": evaluation_metrics,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "status": "completed"
            }
            print(f"ç”ŸæˆæŠ¥å‘Š: {report}")
            return report
        
        # 3. åˆ›å»ºå·¥ä½œæµä»»åŠ¡
        load_task = Task(
            name="load_data",
            func=data_loading_func,
            retry_on_failure=True,
            max_retries=2
        )
        
        process_task = Task(
            name="process_data",
            func=data_processing_func,
            depends_on=["load_data"]
        )
        
        train_task = Task(
            name="train_model",
            func=model_training_func,
            depends_on=["process_data"]
        )
        
        evaluate_task = Task(
            name="evaluate_model",
            func=model_evaluation_func,
            depends_on=["train_model"]
        )
        
        report_task = Task(
            name="report_results",
            func=result_reporting_func,
            depends_on=["evaluate_model"]
        )
        
        # 4. æ·»åŠ ä»»åŠ¡åˆ°å·¥ä½œæµæ„å»ºå™¨
        builder.add_task(load_task)
        builder.add_task(process_task)
        builder.add_task(train_task)
        builder.add_task(evaluate_task)
        builder.add_task(report_task)
        
        # 5. æ„å»ºå·¥ä½œæµ
        print("æ„å»ºè‡ªå®šä¹‰å·¥ä½œæµ")
        custom_workflow = builder.build()
        
        # 6. å¯è§†åŒ–å·¥ä½œæµï¼ˆå‡è®¾æœ‰å¯è§†åŒ–æ¨¡å—ï¼‰
        print("\nå·¥ä½œæµä»»åŠ¡ä¾èµ–å…³ç³»:")
        # ç®€å•æ–‡æœ¬å±•ç¤ºä¾èµ–å…³ç³»
        for task_name, task_obj in custom_workflow.tasks.items():
            dependencies = task_obj.depends_on if task_obj.depends_on else []
            dependency_str = ", ".join(dependencies) if dependencies else "æ— "
            print(f"  ä»»åŠ¡: {task_name}, ä¾èµ–äº: {dependency_str}")
        
        # 7. æ‰§è¡Œè‡ªå®šä¹‰å·¥ä½œæµ
        print("\nå¼€å§‹æ‰§è¡Œè‡ªå®šä¹‰å·¥ä½œæµ...")
        result = custom_workflow.run()
        
        # 8. æ€»ç»“å·¥ä½œæµæ‰§è¡Œç»“æœ
        print("\nè‡ªå®šä¹‰å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {custom_workflow.execution_time:.2f}ç§’")
        print(f"ä»»åŠ¡æ‰§è¡Œé¡ºåº: {', '.join(custom_workflow.execution_order)}")
        
        # ç»“æœåˆ†æ
        final_report = result["report_results"]
        print("\nå·¥ä½œæµæœ€ç»ˆæŠ¥å‘Š:")
        print(f"å‡†ç¡®ç‡: {final_report['metrics']['base_accuracy']:.2f}")
        print(f"ç²¾ç¡®ç‡: {final_report['metrics']['precision']:.2f}")
        print(f"å¬å›ç‡: {final_report['metrics']['recall']:.2f}")
        print(f"F1åˆ†æ•°: {final_report['metrics']['f1']:.2f}")
        print(f"å®Œæˆæ—¶é—´: {final_report['timestamp']}")
        
    except Exception as e:
        print(f"è‡ªå®šä¹‰å·¥ä½œæµæ„å»ºé”™è¯¯: {e}")

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰å·¥ä½œæµç¤ºä¾‹
    run_all()
3.1 æ•°æ®åŠ è½½ä»»åŠ¡
        @workflow.task(name="load_data")
        def load_classification_data(input_path):
            print("æ‰§è¡Œæ•°æ®åŠ è½½ä»»åŠ¡...")
            # æ¨¡æ‹ŸåŠ è½½åˆ†ç±»æ•°æ®
            sample_data = [
                {"id": 1, "text": "è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼Œå¼ºçƒˆæ¨èï¼"},
                {"id": 2, "text": "ä½¿ç”¨ä½“éªŒä¸€èˆ¬ï¼Œæœ‰å¾…æ”¹è¿›"},
                {"id": 3, "text": "å®Œå…¨ä¸ç¬¦åˆé¢„æœŸï¼Œå¤ªå¤±æœ›äº†"},
                {"id": 4, "text": "ç‰©è¶…æ‰€å€¼ï¼Œå¾ˆæ»¡æ„è¿™æ¬¡è´­ä¹°"}
            ]
            print(f"åŠ è½½äº†{len(sample_data)}æ¡æ•°æ®")
            return sample_data
        
        # 3.2 æ•°æ®é¢„å¤„ç†ä»»åŠ¡
        @workflow.task(name="preprocess_data", depends_on=["load_data"])
        def preprocess_classification_data(data):
            print("æ‰§è¡Œæ•°æ®é¢„å¤„ç†ä»»åŠ¡...")
            processor = data_processor.DataProcessor()
            
            processed_data = []
            for item in data:
                processed_text = processor.process_text(
                    item["text"],
                    lowercase=True,
                    remove_special_chars=True
                )
                processed_data.append({
                    "id": item["id"],
                    "original_text": item["text"],
                    "processed_text": processed_text
                })
            
            print(f"é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†äº†{len(processed_data)}æ¡æ•°æ®")
            return processed_data
        
        # 3.3 æ¨¡å‹æ¨ç†ä»»åŠ¡
        @workflow.task(name="model_inference", depends_on=["preprocess_data"])
        def run_classification_model(data):
            print("æ‰§è¡Œæ¨¡å‹æ¨ç†ä»»åŠ¡...")
            
            # åŠ è½½æ¨¡å‹é…ç½®
            model_config = config.ModelConfig(
                model_name=workflow_config["model"]["name"],
                model_path=workflow_config["model"]["path"],
                max_length=workflow_config["model"]["max_length"]
            )
            
            # åˆå§‹åŒ–æ¨¡å‹ï¼ˆè¿™é‡Œæ¨¡æ‹Ÿæ¨¡å‹æ¨ç†ï¼‰
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½çœŸå®çš„æ¨¡å‹å¹¶æ‰§è¡Œæ¨ç†
            classification_results = []
            
            print("æ¨¡å‹æ¨ç†ä¸­...")
            for item in data:
                # æ¨¡æ‹Ÿåˆ†ç±»ç»“æœ
                if "å¥½" in item["original_text"] or "æ¨è" in item["original_text"] or "æ»¡æ„" in item["original_text"]:
                    sentiment = "positive"
                    confidence = 0.92
                elif "ä¸€èˆ¬" in item["original_text"] or "å¾…æ”¹è¿›" in item["original_text"]:
                    sentiment = "neutral"
                    confidence = 0.78
                else:
                    sentiment = "negative"
                    confidence = 0.85
                
                classification_results.append({
                    "id": item["id"],
                    "text": item["original_text"],
                    "predicted_class": sentiment,
                    "confidence": confidence
                })
            
            print(f"æ¨ç†å®Œæˆï¼Œç”Ÿæˆäº†{len(classification_results)}æ¡åˆ†ç±»ç»“æœ")
            return classification_results
        
        # 3.4 ç»“æœåå¤„ç†ä»»åŠ¡
        @workflow.task(name="post_process_results", depends_on=["model_inference"])
        def post_process_classification_results(results):
            print("æ‰§è¡Œç»“æœåå¤„ç†ä»»åŠ¡...")
            
            processor = post_processor.PostProcessor()
            
            # ç­›é€‰é«˜ç½®ä¿¡åº¦ç»“æœ
            filtered_results = processor.rank_and_filter(
                results,
                score_key='confidence',
                min_score=0.8
            )
            
            # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
            processed_results = []
            for result in filtered_results:
                processed_results.append({
                    **result,
                    "processed_at": time.strftime("%Y-%m-%d %H:%M:%S")
                })
            
            print(f"åå¤„ç†å®Œæˆï¼Œä¿ç•™äº†{len(processed_results)}æ¡é«˜ç½®ä¿¡åº¦ç»“æœ")
            return processed_results
        
        # 3.5 ç»“æœä¿å­˜ä»»åŠ¡
        @workflow.task(name="save_results", depends_on=["post_process_results"])
        def save_classification_results(processed_results):
            print("æ‰§è¡Œç»“æœä¿å­˜ä»»åŠ¡...")
            
            output_path = workflow_config["output"]["path"]
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¿å­˜ç»“æœ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(processed_results, f, ensure_ascii=False, indent=2)
            
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            return {"status": "success", "output_path": output_path}
        
        # 4. æ‰§è¡Œå·¥ä½œæµ
        print("\nå¼€å§‹æ‰§è¡Œæ–‡æœ¬åˆ†ç±»å·¥ä½œæµ...")
        result = workflow.run(input_path="./data/sample_texts.csv")
        
        # 5. æ€»ç»“å·¥ä½œæµæ‰§è¡Œç»“æœ
        print("\næ–‡æœ¬åˆ†ç±»å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {workflow.execution_time:.2f}ç§’")
        print(f"ä»»åŠ¡æ‰§è¡Œé¡ºåº: {', '.join(workflow.execution_order)}")
        
        # æ˜¾ç¤ºæœ€ç»ˆåˆ†ç±»ç»“æœ
        print("\nåˆ†ç±»ç»“æœç¤ºä¾‹:")
        for i, item in enumerate(result):
            if i >= 2:  # åªæ˜¾ç¤ºå‰2æ¡ç»“æœ
                break
            print(f"æ ·æœ¬{item['id']}:")
            print(f"  æ–‡æœ¬: {item['text']}")
            print(f"  åˆ†ç±»: {item['predicted_class']}")
            print(f"  ç½®ä¿¡åº¦: {item['confidence']}")
        
    except WorkflowError as e:
        print(f"å·¥ä½œæµé”™è¯¯: {e}")
    except Exception as e:
        print(f"æœªé¢„æœŸé”™è¯¯: {e}")

def run_data_analysis_workflow():
    """
    æ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹
    
    å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªæ•°æ®åˆ†æå’Œå¯è§†åŒ–çš„å®Œæ•´æµç¨‹
    """
    print("\n--- æ•°æ®åˆ†æå·¥ä½œæµç¤ºä¾‹ ---")
    
    try:
        # 1. å®šä¹‰å·¥ä½œæµé…ç½®
        workflow_config = {
            "name": "data_analysis_workflow",
            "description": "æ•°æ®åˆ†æä¸å¯è§†åŒ–å·¥ä½œæµ",
            "data": {
                "input_format": "csv",
                "input_path": "./data/sales_data.csv"
            },
            "analysis": {
                "metrics": ["sum", "mean", "median", "trend"],
                "group_by": "product_category"
            },
            "visualization": {
                "charts": ["bar", "line", "pie"],
                "output_dir": "./output/charts/"
            }
        }
        
        print("åˆå§‹åŒ–æ•°æ®åˆ†æå·¥ä½œæµ")
        
        # 2. åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = workflow_manager.WorkflowManager(workflow_config)
        
        # 3. å®šä¹‰å’Œæ·»åŠ å·¥ä½œæµä»»åŠ¡
        
        # 3.1 æ•°æ®åŠ è½½ä»»åŠ¡
        @workflow.task(name="load_data")
        def load_analysis_data():
            print("æ‰§è¡Œæ•°æ®åŠ è½½ä»»åŠ¡...")
            
            # æ¨¡æ‹Ÿé”€å”®æ•°æ®
            sample_data = {
                "product_category": ["ç”µå­äº§å“", "å®¶å±…ç”¨å“", "é£Ÿå“é¥®æ–™", "ç”µå­äº§å“", "å®¶å±…ç”¨å“", 
                                    "é£Ÿå“é¥®æ–™", "ç”µå­äº§å“", "å®¶å±…ç”¨å“", "é£Ÿå“é¥®æ–™"],
                "product_name": ["æ™ºèƒ½æ‰‹æœº", "æ²™å‘", "çŸ¿æ³‰æ°´", "ç¬”è®°æœ¬ç”µè„‘", "åºŠå«", 
                                "å·§å…‹åŠ›", "å¹³æ¿ç”µè„‘", "é¤æ¡Œ", "å’–å•¡"],
                "sales_amount": [12000, 8500, 1200, 15000, 6800, 900, 8000, 7500, 1500],
                "sales_date": ["2023-01-15", "2023-01-20", "2023-01-25", "2023-02-10", 
                              "2023-02-15", "2023-02-20", "2023-03-05", "2023-03-10", "2023-03-15"]
            }
            
            # è½¬æ¢ä¸ºDataFrameï¼ˆå®é™…ä½¿ç”¨ä¸­é€šå¸¸ä¼šä»æ–‡ä»¶åŠ è½½ï¼‰
            sales_df = pd.DataFrame(sample_data)
            print(f"åŠ è½½äº†é”€å”®æ•°æ®: {len(sales_df)}è¡Œ, {len(sales_df.columns)}åˆ—")
            
            return sales_df
        
        # 3.2 æ•°æ®å¤„ç†ä»»åŠ¡
        @workflow.task(name="process_data", depends_on=["load_data"])
        def process_analysis_data(df):
            print("æ‰§è¡Œæ•°æ®å¤„ç†ä»»åŠ¡...")
            
            # è½¬æ¢æ—¥æœŸåˆ—
            df['sales_date'] = pd.to_datetime(df['sales_date'])
            
            # æ·»åŠ æœˆä»½åˆ—
            df['sales_month'] = df['sales_date'].dt.strftime('%Y-%m')
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_values = df.isnull().sum().sum()
            print(f"æ•°æ®ä¸­çš„ç¼ºå¤±å€¼æ•°é‡: {missing_values}")
            
            # æ·»åŠ ä¸€äº›æ´¾ç”Ÿç‰¹å¾
            df['price_category'] = pd.cut(
                df['sales_amount'], 
                bins=[0, 5000, 10000, float('inf')],
                labels=['ä½ä»·', 'ä¸­ä»·', 'é«˜ä»·']
            )
            
            print("æ•°æ®å¤„ç†å®Œæˆ")
            return df
        
        # 3.3 åˆ†æä»»åŠ¡
        @workflow.task(name="analyze_data", depends_on=["process_data"])
        def analyze_data(df):
            print("æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡...")
            
            results = {}
            
            # æŒ‰äº§å“ç±»åˆ«åˆ†ç»„åˆ†æ
            group_by = workflow_config["analysis"]["group_by"]
            print(f"æŒ‰{group_by}åˆ†ç»„åˆ†æ...")
            
            # è®¡ç®—å„ç±»åˆ«é”€å”®æ€»é¢
            category_sales = df.groupby(group_by)['sales_amount'].sum().to_dict()
            results['category_sales'] = category_sales
            
            # è®¡ç®—å„ç±»åˆ«å¹³å‡é”€å”®é¢
            category_avg_sales = df.groupby(group_by)['sales_amount'].mean().to_dict()
            results['category_avg_sales'] = category_avg_sales
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
            results['total_sales'] = df['sales_amount'].sum()
            results['avg_sale'] = df['sales_amount'].mean()
            results['max_sale'] = df['sales_amount'].max()
            results['min_sale'] = df['sales_amount'].min()
            
            # æŒ‰æœˆä»½ç»Ÿè®¡é”€å”®è¶‹åŠ¿
            monthly_trend = df.groupby('sales_month')['sales_amount'].sum().to_dict()
            results['monthly_trend'] = monthly_trend
            
            print("æ•°æ®åˆ†æå®Œæˆ")
            return results
        
        # 3.4 å¯è§†åŒ–ä»»åŠ¡
        @workflow.task(name="visualize_data", depends_on=["analyze_data"])
        def visualize_data(analysis_results):
            print("æ‰§è¡Œæ•°æ®å¯è§†åŒ–ä»»åŠ¡...")
            
            # å¯¼å…¥å¯è§†åŒ–æ¨¡å—ï¼ˆç¤ºä¾‹ä»£ç ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥æœ‰çœŸå®å®ç°ï¼‰
            from myproject.extensions import visualization
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = workflow_config["visualization"]["output_dir"]
            os.makedirs(output_dir, exist_ok=True)
            
            visualization_results = []
            
            # 1. åˆ›å»ºäº§å“ç±»åˆ«é”€å”®æŸ±çŠ¶å›¾
            category_sales = analysis_results['category_sales']
            bar_chart_path = os.path.join(output_dir, "category_sales_bar.png")
            
            # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿç”Ÿæˆå›¾è¡¨ï¼Œå®é™…é¡¹ç›®ä¸­ä¼šè°ƒç”¨çœŸå®çš„å›¾è¡¨ç”Ÿæˆå‡½æ•°
            print(f"ç”Ÿæˆäº§å“ç±»åˆ«é”€å”®æŸ±çŠ¶å›¾: {bar_chart_path}")
            # visualization.create_bar_chart(
            #     labels=list(category_sales.keys()),
            #     values=list(category_sales.values()),
            #     title="å„äº§å“ç±»åˆ«é”€å”®æ€»é¢",
            #     output_path=bar_chart_path
            # )
            visualization_results.append({
                "type": "bar_chart",
                "title": "å„äº§å“ç±»åˆ«é”€å”®æ€»é¢",
                "path": bar_chart_path
            })
            
            # 2. åˆ›å»ºæœˆåº¦é”€å”®è¶‹åŠ¿çº¿å›¾
            monthly_trend = analysis_results['monthly_trend']
            line_chart_path = os.path.join(output_dir, "monthly_sales_trend.png")
            
            print(f"ç”Ÿæˆæœˆåº¦é”€å”®è¶‹åŠ¿çº¿å›¾: {line_chart_path}")
            # visualization.create_line_chart(
            #     labels=list(monthly_trend.keys()),
            #     values=list(monthly_trend.values()),
            #     title="æœˆåº¦é”€å”®è¶‹åŠ¿",
            #     output_path=line_chart_path
            # )
            visualization_results.append({
                "type": "line_chart",
                "title": "æœˆåº¦é”€å”®è¶‹åŠ¿",
                "path": line_chart_path
            })
            
            # 3. åˆ›å»ºäº§å“ç±»åˆ«é”€å”®å æ¯”é¥¼å›¾
            pie_chart_path = os.path.join(output_dir, "category_sales_pie.png")
            
            print(f"ç”Ÿæˆäº§å“ç±»åˆ«é”€å”®å æ¯”é¥¼å›¾: {pie_chart_path}")
            # visualization.create_pie_chart(
            #     labels=list(category_sales.keys()),
            #     values=list(category_sales.values()),
            #     title="å„äº§å“ç±»åˆ«é”€å”®å æ¯”",
            #     output_path=pie_chart_path
            # )
            visualization_results.append({
                "type": "pie_chart",
                "title": "å„äº§å“ç±»åˆ«é”€å”®å æ¯”",
                "path": pie_chart_path
            })
            
            print(f"å¯è§†åŒ–ä»»åŠ¡å®Œæˆï¼Œç”Ÿæˆäº†{len(visualization_results)}ä¸ªå›¾è¡¨")
            return visualization_results
        
        # 3.5 æŠ¥å‘Šç”Ÿæˆä»»åŠ¡
        @workflow.task(name="generate_report", depends_on=["analyze_data", "visualize_data"])
        def generate_analysis_report(analysis_results, visualization_results):
            print("æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡...")
            
            # ç”ŸæˆæŠ¥å‘Šçš„æ–‡ä»¶è·¯å¾„
            report_path = os.path.join(
                workflow_config["visualization"]["output_dir"],
                "sales_analysis_report.html"
            )
            
            # å‡†å¤‡æŠ¥å‘Šå†…å®¹
            report_content = f"""
            <html>
            <head>
                <title>é”€å”®æ•°æ®åˆ†ææŠ¥å‘Š</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    h1 {{ color: #2c3e50; }}
                    .summary {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; }}
                    .chart-container {{ margin: 20px 0; }}
                </style>
            </head>
            <body>
                <h1>é”€å”®æ•°æ®åˆ†ææŠ¥å‘Š</h1>
                <div class="summary">
                    <h2>é”€å”®æ€»è§ˆ</h2>
                    <p>æ€»é”€å”®é¢: {analysis_results['total_sales']} å…ƒ</p>
                    <p>å¹³å‡é”€å”®é¢: {analysis_results['avg_sale']:.2f} å…ƒ</p>
                    <p>æœ€é«˜é”€å”®é¢: {analysis_results['max_sale']} å…ƒ</p>
                    <p>æœ€ä½é”€å”®é¢: {analysis_results['min_sale']} å…ƒ</p>
                </div>
                
                <h2>äº§å“ç±»åˆ«åˆ†æ</h2>
                <div class="chart-container">
                    <h3>å„äº§å“ç±»åˆ«é”€å”®æ€»é¢</h3>
                    <img src="category_sales_bar.png" alt="äº§å“ç±»åˆ«é”€å”®æŸ±çŠ¶å›¾">
                    
                    <h3>äº§å“ç±»åˆ«é”€å”®å æ¯”</h3>
                    <img src="category_sales_pie.png" alt="äº§å“ç±»åˆ«é”€å”®å æ¯”é¥¼å›¾">
                </div>
                
                <h2>é”€å”®è¶‹åŠ¿åˆ†æ</h2>
                <div class="chart-container">
                    <h3>æœˆåº¦é”€å”®è¶‹åŠ¿</h3>
                    <img src="monthly_sales_trend.png" alt="æœˆåº¦é”€å”®è¶‹åŠ¿çº¿å›¾">
                </div>
                
                <h2>ç±»åˆ«è¯¦ç»†åˆ†æ</h2>
                <table border="1" cellpadding="5">
                    <tr>
                        <th>äº§å“ç±»åˆ«</th>
                        <th>æ€»é”€å”®é¢</th>
                        <th>å¹³å‡é”€å”®é¢</th>
                    </tr>
            """
            
            # æ·»åŠ å„äº§å“ç±»åˆ«çš„è¯¦ç»†æ•°æ®
            for category in analysis_results['category_sales']:
                report_content += f"""
                    <tr>
                        <td>{category}</td>
                        <td>{analysis_results['category_sales'][category]} å…ƒ</td>
                        <td>{analysis_results['category_avg_sales'][category]:.2f} å…ƒ</td>
                    </tr>
                """
            
            # å®ŒæˆæŠ¥å‘Šå†…å®¹
            report_content += """
                </table>
                
                <h2>ç»“è®ºä¸å»ºè®®</h2>
                <p>æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š</p>
                <ul>
                    <li>ç”µå­äº§å“ç±»åˆ«çš„é”€å”®é¢æœ€é«˜ï¼Œåº”ç»§ç»­åŠ å¼ºè¯¥ç±»åˆ«çš„è¥é”€</li>
                    <li>é£Ÿå“é¥®æ–™ç±»åˆ«çš„é”€å”®é¢ç›¸å¯¹è¾ƒä½ï¼Œä½†åˆ©æ¶¦ç‡å¯èƒ½è¾ƒé«˜ï¼Œéœ€è¿›ä¸€æ­¥åˆ†æ</li>
                    <li>é”€å”®è¶‹åŠ¿å‘ˆé€æœˆä¸Šå‡ï¼Œè¡¨æ˜è¥é”€ç­–ç•¥æœ‰æ•ˆ</li>
                </ul>
            </body>
            </html>
            """
            
            # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return {"report_path": report_path}
        
        # 4. æ‰§è¡Œå·¥ä½œæµ
        print("\nå¼€å§‹æ‰§è¡Œæ•°æ®åˆ†æå·¥ä½œæµ...")
        result = workflow.run()
        
        # 5. æ€»ç»“å·¥ä½œæµæ‰§è¡Œç»“æœ
        print("\næ•°æ®åˆ†æå·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {workflow.execution_time:.2f}ç§’")
        print(f"ä»»åŠ¡æ‰§è¡Œé¡ºåº: {', '.join(workflow.execution_order)}")
        print(f"åˆ†ææŠ¥å‘Šè·¯å¾„: {result['report_path']}")
        
    except WorkflowError as e:
        print(f"å·¥ä½œæµé”™è¯¯: {e}")
    except Exception as e:
        print(f"æœªé¢„æœŸé”™è¯¯: {e}")

def run_content_generation_workflow():
    """
    å†…å®¹ç”Ÿæˆå·¥ä½œæµç¤ºä¾‹
    
    å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªå†…å®¹ç”Ÿæˆå’Œä¼˜åŒ–çš„å®Œæ•´æµç¨‹
    """
    print("\n--- å†…å®¹ç”Ÿæˆå·¥ä½œæµç¤ºä¾‹ ---")
    
    try:
        # 1. å®šä¹‰å·¥ä½œæµé…ç½®
        workflow_config = {
            "name": "content_generation_workflow",
            "description": "å†…å®¹ç”Ÿæˆä¸ä¼˜åŒ–å·¥ä½œæµ",
            "model": {
                "name": "content-generator",
                "path": "./models/content-generator",
                "max_length": 1024
            },
            "generation": {
                "temperature": 0.7,
                "top_p": 0.9,
                "content_types": ["article", "social_media", "email"]
            },
            "optimization": {
                "readability_check": True,
                "sentiment_analysis": True,
                "grammar_check": True
            }
        }
        
        print("åˆå§‹åŒ–å†…å®¹ç”Ÿæˆå·¥ä½œæµ")
        
        # 2. åˆ›å»ºå·¥ä½œæµç®¡ç†å™¨
        workflow = workflow_manager.WorkflowManager(workflow_config)
        
        # 3. å®šä¹‰å’Œæ·»åŠ å·¥ä½œæµä»»åŠ¡
        
        #